{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BS6207Assi1_demo .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "e0nb5kCumMXu",
        "co1tP8E2nN7h"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cns4d7IC_qqk"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7XD6806AF0U"
      },
      "source": [
        "# randomly generate x,y\n",
        "x = np.random.rand(2)\n",
        "y = (x[0]**2 + x[1]**2)/2\n",
        "\n",
        "# first hidden layer\n",
        "w1 = np.random.rand(10,2)\n",
        "b1 = np.random.rand(10,1)\n",
        "\n",
        "# second hidden layer\n",
        "w2 = np.random.rand(10,10)\n",
        "b2 = np.random.rand(10,1)\n",
        "\n",
        "#output layer\n",
        "w3 = np.random.rand(10)\n",
        "b3 = np.random.rand(10)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0nb5kCumMXu"
      },
      "source": [
        "# Gradient calculation with script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkPHOxY3AX08"
      },
      "source": [
        "def sigmoid(x):\n",
        "  # sigmoid function\n",
        "  return 1 / (1 + math. exp(-x))\n",
        "\n",
        "def forward(x,w,b,act,layer,torch_flag = False):\n",
        "  if layer == 1:\n",
        "    v1 = []\n",
        "    z = x * w + b\n",
        "    ze = 0\n",
        "    for i in range(10):\n",
        "      for j in range(2):\n",
        "        ze += z[i][j]\n",
        "        vv = act(ze)\n",
        "      v1.append(vv)\n",
        "    if torch_flag:\n",
        "      return v1,z\n",
        "    # the format of data type should be change when use pytorch\n",
        "    return np.array(v1),z \n",
        "  \n",
        "  # second layer pass\n",
        "  if layer == 2:\n",
        "    z = x * w + b\n",
        "    v2 = []\n",
        "    ze = 0\n",
        "    for i in range(10):\n",
        "      for j in range(10):\n",
        "        ze += z[i][j]\n",
        "      vv = act(ze)\n",
        "      v2.append(vv)\n",
        "    if torch_flag:\n",
        "      return v2,z\n",
        "    return np.array(v2),z \n",
        "    \n",
        "  # third layer pass\n",
        "  if layer == 3:\n",
        "    if torch_flag:\n",
        "        zz = 0\n",
        "        for i in range(10):\n",
        "          zz += x[i] * w[i] + b[i]\n",
        "        return zz\n",
        "    z = x * w + b\n",
        "    return np.sum(z)\n",
        "\n",
        "def loss(pred,y):\n",
        "  # loss function\n",
        "  L = (pred- y)**2\n",
        "  return L"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrhdKOvYdY8L"
      },
      "source": [
        "# dL/dw3 = 2(yred-y) * v2\n",
        "def gradient_w3(v,y,pred):\n",
        "  dw3 = []\n",
        "  for i in range(10):\n",
        "    dw = 2 *v[i]* (pred - y)\n",
        "    dw3.append(dw)\n",
        "  return np.array(dw3)\n",
        "\n",
        "# dL/db3 = 2(yred-y)\n",
        "def gradient_b3(y,pred):\n",
        "  db3 = []\n",
        "  for i in range(10):\n",
        "    db = 2 * (pred - y)\n",
        "    db3.append(db)\n",
        "  return np.array(db3)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB0Fs64YAf90"
      },
      "source": [
        "#dL/dw2 = 2(yred-y) * w3*sig(z2) * (1-sig(z2)) * v1\n",
        "def gradient_w2(v,y,pred,z2,w3):\n",
        "  dw2 = []\n",
        "  ze = 0\n",
        "  sigz2 = []\n",
        "  for i in range(10):\n",
        "    for j in range(10):\n",
        "        ze += z2[i][j]\n",
        "    sigz = sigmoid(ze) * (1 - sigmoid(ze))\n",
        "    # save the sig(z2), no need to calculate next time\n",
        "    sigz2.append(sigz)\n",
        "    dw = 2 *v[i]* (pred - y) * sigz *w3[i]\n",
        "    dw2.append(dw)\n",
        "  return np.array(dw2),np.array(sigz2)\n",
        "#dL/db2 = 2(yred-y) * w3*sig(z2) * (1-sig(z2))\n",
        "def gradient_b2(y,pred,sigz2,w3):\n",
        "  db2 = []\n",
        "  for i in range(10):\n",
        "    db = 2 * (pred - y)* sigz2[i] *w3[i]\n",
        "    db2.append([db])\n",
        "  return np.array(db2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6noiGyK6DUE"
      },
      "source": [
        "# dL/dw1 = 2 * (yred - y)* w3 *sig(z2) * w2 * sig(z1) * x\n",
        "def gradient_w1(x,y,w2,w3,z1,pred,sigz2):\n",
        "  dw1 = []\n",
        "  sigz1 = []\n",
        "  for i in range(10):\n",
        "    sigz = sigmoid(z1[i][0] + z1[i][1]) * (1 - sigmoid(z1[i][0] + z1[i][1]))\n",
        "    sigz1.append(sigz)\n",
        "    w2sum = np.sum(w2[i])\n",
        "    dwtemp1 = 2 * x[0] * (pred - y) * w3[i] * sigz2[i] * w2sum * sigz\n",
        "    dwtemp2 = 2 * x[1] * (pred - y) * w3[i] * sigz2[i] * w2sum * sigz\n",
        "    dw1.append([dwtemp1,dwtemp2])\n",
        "  return np.array(dw1),np.array(sigz1)\n",
        "\n",
        "# dL/db1 = 2 * (yred - y)* w3 *sig(z2) * w2 * sig(z1)\n",
        "def gradient_b1(y,pred,sigz2,sigz1,w2):\n",
        "  db1 = []\n",
        "  for i in range(10):\n",
        "    w2sum = np.sum(w2[i])\n",
        "    db = 2 * (pred - y)*w3[i]*sigz2[i] * w2sum * sigz1[i]\n",
        "    db1.append([db])\n",
        "  return np.array(db1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyKIYsVoEQFt",
        "outputId": "1cfb9a67-02ae-4a14-9a19-f0fea1ac022a"
      },
      "source": [
        "# forward \n",
        "v1,z1 = forward(x,w1,b1,sigmoid,1)\n",
        "v2,z2 = forward(v1,w2,b2,sigmoid,2)\n",
        "y_pred = forward(v2,w3,b3,sigmoid,3)\n",
        "print('The predicted y is: '+ str(round(y_pred,2)))\n",
        "\n",
        "# loss\n",
        "L = loss(y_pred,y)\n",
        "\n",
        "# gradient decent\n",
        "dw3 = gradient_w3(v2,y,y_pred)\n",
        "db3 = gradient_b3(y,y_pred)\n",
        "dw2,sigz2 = gradient_w2(v1,y,y_pred,z2,w3)\n",
        "db2 = gradient_b2(y,y_pred,sigz2,w3)\n",
        "dw1, sigz1 = gradient_w1(x,y,w2,w3,z1,y_pred,sigz2)\n",
        "db1 = gradient_b1(y,y_pred,sigz2,sigz1,w2)\n",
        "\n",
        "\n",
        "print('-------------dw3------------')\n",
        "print(dw3)\n",
        "print('-------------db3------------')\n",
        "print(db3)\n",
        "print('-------------dw2------------')\n",
        "print(dw2)\n",
        "print('-------------db2------------')\n",
        "print(db2)\n",
        "print('-------------dw1------------')\n",
        "print(dw1)\n",
        "print('-------------db1------------')\n",
        "print(db1)\n",
        "\n",
        "\n",
        "db2 = np.reshape(db2,10)\n",
        "db1 = np.reshape(db1,10)\n",
        "dw1 = np.reshape(dw1,20)\n",
        "dw11 = dw1[:10]\n",
        "dw12 = dw1[10:]\n",
        "\n",
        "output_df = pd.DataFrame({'dw3': dw3,'db3':db3,'dw2': dw2,'db2':db2,'dw11': dw11,'dw12':dw12,'db1':db1}) \n",
        "output_df.to_csv('gradient decent script.dat')\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The predicted y is: 8.36\n",
            "-------------dw3------------\n",
            "[15.58998543 15.59228346 15.59228351 15.59228351 15.59228351 15.59228351\n",
            " 15.59228351 15.59228351 15.59228351 15.59228351]\n",
            "-------------db3------------\n",
            "[15.59228351 15.59228351 15.59228351 15.59228351 15.59228351 15.59228351\n",
            " 15.59228351 15.59228351 15.59228351 15.59228351]\n",
            "-------------dw2------------\n",
            "[8.45728103e-04 4.84804552e-08 5.46632872e-16 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00]\n",
            "-------------db2------------\n",
            "[[9.15226727e-04]\n",
            " [4.88109015e-08]\n",
            " [5.47253208e-16]\n",
            " [0.00000000e+00]\n",
            " [0.00000000e+00]\n",
            " [0.00000000e+00]\n",
            " [0.00000000e+00]\n",
            " [0.00000000e+00]\n",
            " [0.00000000e+00]\n",
            " [0.00000000e+00]]\n",
            "-------------dw1------------\n",
            "[[2.55289484e-04 1.69807009e-04]\n",
            " [1.50851951e-08 1.00339890e-08]\n",
            " [3.47921055e-16 2.31421337e-16]\n",
            " [0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00]]\n",
            "-------------db1------------\n",
            "[[2.87930686e-04]\n",
            " [1.70139816e-08]\n",
            " [3.92406090e-16]\n",
            " [0.00000000e+00]\n",
            " [0.00000000e+00]\n",
            " [0.00000000e+00]\n",
            " [0.00000000e+00]\n",
            " [0.00000000e+00]\n",
            " [0.00000000e+00]\n",
            " [0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co1tP8E2nN7h"
      },
      "source": [
        "# Deap learning process with script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYsEm_xPj-og",
        "outputId": "f23e385b-433e-44e6-80c0-69b349fff31e"
      },
      "source": [
        "learning_rate = 0.01\n",
        "for epoch in range(10):\n",
        "  # forward \n",
        "  v1,z1 = forward(x,w1,b1,sigmoid,1)\n",
        "  v2,z2 = forward(v1,w2,b2,sigmoid,2)\n",
        "  y_pred = forward(v2,w3,b3,sigmoid,3)\n",
        "\n",
        "  # loss\n",
        "  L = loss(y_pred,y)\n",
        "\n",
        "  # gradient decent\n",
        "  dw3 = gradient_w3(v2,y,y_pred)\n",
        "  db3 = gradient_b3(y,y_pred)\n",
        "  dw2,sigz2 = gradient_w2(v1,y,y_pred,z2,w3)\n",
        "  db2 = gradient_b2(y,y_pred,sigz2,w3)\n",
        "  dw1, sigz1 = gradient_w1(x,y,w2,w3,z1,y_pred,sigz2)\n",
        "  db1 = gradient_b1(y,y_pred,sigz2,sigz1,w2)\n",
        "\n",
        "  # update w and b \n",
        "  w3 -= learning_rate * dw3\n",
        "  b3 -= learning_rate * db3\n",
        "  w2 -= learning_rate * dw2\n",
        "  b2 -= learning_rate * db2\n",
        "  w1 -= learning_rate * dw1\n",
        "  b1 -= learning_rate * db1\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    print(f'epoch{epoch + 1}: loss = {L}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch1: loss = 142.64456701911698\n",
            "epoch2: loss = 51.3603192337142\n",
            "epoch3: loss = 18.492701088767955\n",
            "epoch4: loss = 6.658448327141117\n",
            "epoch5: loss = 2.397428867029006\n",
            "epoch6: loss = 0.8632139069983521\n",
            "epoch7: loss = 0.31080723920078707\n",
            "epoch8: loss = 0.11190869243280575\n",
            "epoch9: loss = 0.04029364129859838\n",
            "epoch10: loss = 0.014508055548621534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLQ790kHnkzb"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SgqDqNVozMD"
      },
      "source": [
        "# Gradience calculation with pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCZoy1dev7Fj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f9fda45-014d-40ad-f88e-dc3d160b73c1"
      },
      "source": [
        "pip install torch"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0H0CiCvwa7C"
      },
      "source": [
        "def forward(x,w,b,act,layer,torch_flag = False):\n",
        "  if layer == 1:\n",
        "    v1 = []\n",
        "    z = x * w + b\n",
        "    ze = 0\n",
        "    for i in range(10):\n",
        "      for j in range(2):\n",
        "        ze += z[i][j]\n",
        "        vv = act(ze)\n",
        "      v1.append(vv)\n",
        "    if torch_flag:\n",
        "      return v1,z\n",
        "    # the format of data type should be change when use pytorch\n",
        "    return np.array(v1),z \n",
        "  \n",
        "  # second layer pass\n",
        "  if layer == 2:\n",
        "    z = x * w + b\n",
        "    v2 = []\n",
        "    ze = 0\n",
        "    for i in range(10):\n",
        "      for j in range(10):\n",
        "        ze += z[i][j]\n",
        "      vv = act(ze)\n",
        "      v2.append(vv)\n",
        "    if torch_flag:\n",
        "      return v2,z\n",
        "    return np.array(v2),z \n",
        "    \n",
        "  # third layer pass\n",
        "  if layer == 3:\n",
        "    if torch_flag:\n",
        "        zz = 0\n",
        "        for i in range(10):\n",
        "          zz += x[i] * w[i] + b[i]\n",
        "        return zz\n",
        "    z = x * w + b\n",
        "    return np.sum(z)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Au657bUBMXR",
        "outputId": "ec749748-913d-4a0a-c51d-b9557d785518"
      },
      "source": [
        "# calculate gradient decent with torch\n",
        "import torch\n",
        "x_train = torch.tensor(x,requires_grad=True)\n",
        "y_train = torch.tensor(y,requires_grad=True)\n",
        "\n",
        "tw1 = torch.tensor(w1, requires_grad=True)\n",
        "tb1 = torch.tensor(b1, requires_grad=True)\n",
        "tw2 = torch.tensor(w2, requires_grad=True)\n",
        "tb2 = torch.tensor(b2, requires_grad=True)\n",
        "tw3 = torch.tensor(w3, requires_grad=True)\n",
        "tb3 = torch.tensor(b3, requires_grad=True)\n",
        "\n",
        "# active function\n",
        "m = torch.nn.Sigmoid()\n",
        "# forward \n",
        "v1,z = forward(x_train,tw1,tb1,m,1,True)\n",
        "tv1 = torch.stack(v1)\n",
        "v2,z = forward(tv1,tw2,tb2,m,2,True)\n",
        "tpred = forward(v2,tw3,tb3,m,3,True)\n",
        "\n",
        "# loss\n",
        "L = loss(tpred,y_train)\n",
        "# backward\n",
        "L.backward()\n",
        "\n",
        "print('The predicted y is:')\n",
        "print(tpred)\n",
        "print('-------------dw3------------')\n",
        "print(tw3.grad)\n",
        "print('-------------db3------------')\n",
        "print(tb3.grad)\n",
        "print('-------------dw2------------')\n",
        "print(tw2.grad)\n",
        "print('-------------db2------------')\n",
        "print(tb2.grad)\n",
        "print('-------------dw1------------')\n",
        "print(tw1.grad)\n",
        "print('-------------db1------------')\n",
        "print(tb1.grad)\n",
        "\n",
        "torch_df = pd.DataFrame({'dw3': tw3.grad,'db3':tb3.grad,'dw2': tw2.grad,\n",
        "                         'db2':tb2.grad,'dw1': tw1.grad,'db1':tb1.grad,}) \n",
        "torch_df.to_csv('gradient decent torch.dat')\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The predicted y is:\n",
            "tensor(8.3631, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "-------------dw3------------\n",
            "tensor([15.5900, 15.5923, 15.5923, 15.5923, 15.5923, 15.5923, 15.5923, 15.5923,\n",
            "        15.5923, 15.5923], dtype=torch.float64)\n",
            "-------------db3------------\n",
            "tensor([15.5923, 15.5923, 15.5923, 15.5923, 15.5923, 15.5923, 15.5923, 15.5923,\n",
            "        15.5923, 15.5923], dtype=torch.float64)\n",
            "-------------dw2------------\n",
            "tensor([[8.4577e-04, 9.0908e-04, 9.1424e-04, 9.1515e-04, 9.1526e-04, 9.1527e-04,\n",
            "         9.1528e-04, 9.1528e-04, 9.1528e-04, 9.1528e-04],\n",
            "        [4.5104e-08, 4.8480e-08, 4.8756e-08, 4.8804e-08, 4.8810e-08, 4.8811e-08,\n",
            "         4.8811e-08, 4.8811e-08, 4.8811e-08, 4.8811e-08],\n",
            "        [5.0570e-16, 5.4355e-16, 5.4663e-16, 5.4718e-16, 5.4725e-16, 5.4725e-16,\n",
            "         5.4725e-16, 5.4725e-16, 5.4725e-16, 5.4725e-16],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], dtype=torch.float64)\n",
            "-------------db2------------\n",
            "tensor([[9.1528e-03],\n",
            "        [4.8811e-07],\n",
            "        [5.4725e-15],\n",
            "        [0.0000e+00],\n",
            "        [0.0000e+00],\n",
            "        [0.0000e+00],\n",
            "        [0.0000e+00],\n",
            "        [0.0000e+00],\n",
            "        [0.0000e+00],\n",
            "        [0.0000e+00]], dtype=torch.float64)\n",
            "-------------dw1------------\n",
            "tensor([[4.0204e-05, 2.6742e-05],\n",
            "        [1.7110e-06, 1.1380e-06],\n",
            "        [3.1731e-07, 2.1106e-07],\n",
            "        [5.6854e-08, 3.7817e-08],\n",
            "        [3.2313e-09, 2.1493e-09],\n",
            "        [5.2458e-10, 3.4893e-10],\n",
            "        [2.9387e-11, 1.9547e-11],\n",
            "        [1.9437e-12, 1.2929e-12],\n",
            "        [4.4192e-13, 2.9395e-13],\n",
            "        [1.6579e-14, 1.1027e-14]], dtype=torch.float64)\n",
            "-------------db1------------\n",
            "tensor([[9.0690e-05],\n",
            "        [3.8594e-06],\n",
            "        [7.1577e-07],\n",
            "        [1.2825e-07],\n",
            "        [7.2889e-09],\n",
            "        [1.1833e-09],\n",
            "        [6.6288e-11],\n",
            "        [4.3845e-12],\n",
            "        [9.9686e-13],\n",
            "        [3.7397e-14]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlDkL4AmpEyq"
      },
      "source": [
        "# Deep learning process with pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NdhEyqYj2x7",
        "outputId": "b1923d22-b398-4749-c7b0-acf6339b0d11"
      },
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(0)\n",
        "x_train = torch.rand(1,2)\n",
        "y_train = torch.empty(1,1)\n",
        "y_train[0,0] = (x_train[0][0]**2 + x_train[0][1]**2)/2\n",
        "\n",
        "\n",
        "class DLmodel(torch.nn.Module):\n",
        "    # build the class the this 2 hidden layer deep learning model\n",
        "    def __init__(self):\n",
        "        # initiate the model with corresponding input and output\n",
        "        super(DLmodel, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(2,10)\n",
        "        self.linear2 = torch.nn.Linear(10,10)\n",
        "        self.linear3 = torch.nn.Linear(10,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # forward pass\n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        h1_sigmoid = sigmoid(self.linear1(x_train))\n",
        "        h2_sigmoid = sigmoid(self.linear2(h1_sigmoid))   \n",
        "        y_pred = self.linear3(h2_sigmoid)\n",
        "        return y_pred\n",
        "\n",
        "# construct model\n",
        "model = DLmodel()\n",
        "\n",
        "# loss function\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "# optimising process\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "for t in range(15):\n",
        "  # forward pass\n",
        "  y_pred = model(x_train)\n",
        "  # loss\n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "  print('step %d: loss=%f   ' % (t, loss.item()))   \n",
        "  # zero gradientssuch that the gradient could not be accumulated\n",
        "  optimizer.zero_grad()\n",
        "  # perform a backward pass\n",
        "  loss.backward()\n",
        "  #update the weights\n",
        "  optimizer.step()\n",
        "\n",
        "'''This is how you can print the weight and bias of each layer, but not know how to save it properly'''\n",
        "print(y)\n",
        "print(y_pred)\n",
        "print(model.linear3.bias.grad)\n",
        "print(model.linear3.weight.grad)\n",
        "print(model.linear2.bias.grad)\n",
        "print(model.linear2.weight.grad)\n",
        "print(model.linear1.bias.grad)\n",
        "print(model.linear1.weight.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0: loss=0.986679   \n",
            "step 1: loss=0.064272   \n",
            "step 2: loss=0.004507   \n",
            "step 3: loss=0.000315   \n",
            "step 4: loss=0.000022   \n",
            "step 5: loss=0.000002   \n",
            "step 6: loss=0.000000   \n",
            "step 7: loss=0.000000   \n",
            "step 8: loss=0.000000   \n",
            "step 9: loss=0.000000   \n",
            "step 10: loss=0.000000   \n",
            "step 11: loss=0.000000   \n",
            "step 12: loss=0.000000   \n",
            "step 13: loss=0.000000   \n",
            "step 14: loss=0.000000   \n",
            "tensor([-1.1921e-07])\n",
            "tensor([[-4.1819e-08, -5.9634e-08, -6.5727e-08, -6.3175e-08, -7.6438e-08,\n",
            "         -5.6411e-08, -6.1303e-08, -5.3890e-08, -7.1631e-08, -4.9985e-08]])\n",
            "tensor([-1.0607e-08, -3.3048e-09,  3.9322e-09,  1.9669e-10, -1.0532e-08,\n",
            "        -3.6911e-09,  5.3326e-10,  3.5020e-09,  3.7761e-09,  4.4017e-09])\n",
            "tensor([[-3.2148e-09, -6.6929e-09, -6.1228e-09, -5.8128e-09, -6.4215e-09,\n",
            "         -2.1870e-09, -3.8417e-09, -6.0604e-09, -3.3973e-09, -8.1288e-09],\n",
            "        [-1.0016e-09, -2.0852e-09, -1.9076e-09, -1.8110e-09, -2.0007e-09,\n",
            "         -6.8137e-10, -1.1969e-09, -1.8882e-09, -1.0585e-09, -2.5326e-09],\n",
            "        [ 1.1918e-09,  2.4811e-09,  2.2698e-09,  2.1549e-09,  2.3805e-09,\n",
            "          8.1073e-10,  1.4242e-09,  2.2467e-09,  1.2594e-09,  3.0134e-09],\n",
            "        [ 5.9611e-11,  1.2410e-10,  1.1353e-10,  1.0779e-10,  1.1907e-10,\n",
            "          4.0553e-11,  7.1235e-11,  1.1238e-10,  6.2995e-11,  1.5073e-10],\n",
            "        [-3.1919e-09, -6.6453e-09, -6.0792e-09, -5.7715e-09, -6.3758e-09,\n",
            "         -2.1714e-09, -3.8144e-09, -6.0173e-09, -3.3731e-09, -8.0709e-09],\n",
            "        [-1.1187e-09, -2.3290e-09, -2.1306e-09, -2.0227e-09, -2.2346e-09,\n",
            "         -7.6103e-10, -1.3368e-09, -2.1089e-09, -1.1822e-09, -2.8286e-09],\n",
            "        [ 1.6162e-10,  3.3647e-10,  3.0781e-10,  2.9223e-10,  3.2283e-10,\n",
            "          1.0995e-10,  1.9313e-10,  3.0468e-10,  1.7079e-10,  4.0866e-10],\n",
            "        [ 1.0614e-09,  2.2096e-09,  2.0214e-09,  1.9191e-09,  2.1200e-09,\n",
            "          7.2203e-10,  1.2683e-09,  2.0008e-09,  1.1216e-09,  2.6837e-09],\n",
            "        [ 1.1444e-09,  2.3826e-09,  2.1796e-09,  2.0693e-09,  2.2860e-09,\n",
            "          7.7854e-10,  1.3676e-09,  2.1574e-09,  1.2094e-09,  2.8937e-09],\n",
            "        [ 1.3341e-09,  2.7774e-09,  2.5408e-09,  2.4122e-09,  2.6648e-09,\n",
            "          9.0754e-10,  1.5942e-09,  2.5149e-09,  1.4098e-09,  3.3732e-09]])\n",
            "tensor([-3.5921e-10,  5.2183e-10, -3.0542e-10, -5.2185e-10, -1.7052e-10,\n",
            "        -4.2354e-10, -7.7254e-10, -1.3303e-10,  4.4017e-10, -3.1037e-10])\n",
            "tensor([[-1.7826e-10, -2.7596e-10],\n",
            "        [ 2.5896e-10,  4.0088e-10],\n",
            "        [-1.5156e-10, -2.3463e-10],\n",
            "        [-2.5897e-10, -4.0090e-10],\n",
            "        [-8.4620e-11, -1.3099e-10],\n",
            "        [-2.1019e-10, -3.2538e-10],\n",
            "        [-3.8338e-10, -5.9349e-10],\n",
            "        [-6.6017e-11, -1.0220e-10],\n",
            "        [ 2.1844e-10,  3.3815e-10],\n",
            "        [-1.5402e-10, -2.3843e-10]])\n",
            "0.362046981774819\n",
            "tensor([[0.4182]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}